{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK 7 \n",
    "### Elements of Computing \n",
    "### Sabrina Cohen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens: ['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n",
      "\n",
      " Token | Lemma | Head | Morph:\n",
      "The - Lemma: the | Head: fox | Morph: Definite=Def|PronType=Art\n",
      "quick - Lemma: quick | Head: fox | Morph: Degree=Pos\n",
      "brown - Lemma: brown | Head: fox | Morph: Degree=Pos\n",
      "fox - Lemma: fox | Head: jump | Morph: Number=Sing\n",
      "does - Lemma: do | Head: jump | Morph: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "n't - Lemma: not | Head: jump | Morph: Polarity=Neg\n",
      "jump - Lemma: jump | Head: jump | Morph: VerbForm=Inf\n",
      "over - Lemma: over | Head: jump | Morph: \n",
      "the - Lemma: the | Head: dog | Morph: Definite=Def|PronType=Art\n",
      "lazy - Lemma: lazy | Head: dog | Morph: Degree=Pos\n",
      "dog - Lemma: dog | Head: over | Morph: Number=Sing\n",
      ". - Lemma: . | Head: jump | Morph: PunctType=Peri\n",
      "Natural - Lemma: Natural | Head: Language | Morph: Number=Sing\n",
      "Language - Lemma: Language | Head: Processing | Morph: Number=Sing\n",
      "Processing - Lemma: processing | Head: is | Morph: Number=Sing\n",
      "is - Lemma: be | Head: is | Morph: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "fascinating - Lemma: fascinating | Head: is | Morph: Degree=Pos\n",
      "! - Lemma: ! | Head: is | Morph: PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Input text\n",
    "text = (\n",
    "    \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
    ")\n",
    "\n",
    "# Apply the spaCy pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and print token list \n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(\" Tokens:\", tokens_spacy)\n",
    "\n",
    "# Detailed token analysis\n",
    "print (\"\\n Token | Lemma | Head | Morph:\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - Lemma: {token.lemma_} | Head: {token.head.text} | Morph: {token.morph}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* How does spaCy process the various tokens? (Hint: Loop through the doc container using the token attributes: .text_, .head, .lemma_, .morph.)\n",
    "    - SpaCy processes the text by breaking it down into the individual words, which are known as token objects. Each token has unique attributes, and analyzing them alllows us to figure out the purpose anfd characteristics of each word within that sentence.  In this case, the code displayed the following attributes:\n",
    "        - Text: The entity as it appears in the text.\n",
    "        - Lemma: The base/dictionary form of a word.\n",
    "        - Head: The main word another word depends on.\n",
    "        - Morph: Grammatical features like tense, number, or gender.\n",
    "* How does spaCy handle punctuation marks like periods and commas?\n",
    "    - SpaCy handles punctuation marks like their own token, the same way as words are listed. This is important as it also analyzes these small marks, and displays their unique attributes - \n",
    "even though they aren't complete words. \n",
    "* What happens when the text includes contractions (e.g., \"don't\")?\n",
    "    -   When looking at how spaCy proesses the various tokens, it is clear that it separates contractions into two. In this case, one token was 'does' and the other was 'n't'. If analyzed closely, the 'n't' token is processed as a negation, so NLP understands the grammatical structure and accurately analyzes this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Part-of-Speech Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - the: DET\n",
      "quick - quick: ADJ\n",
      "brown - brown: ADJ\n",
      "fox - fox: NOUN\n",
      "does - do: AUX\n",
      "n't - not: PART\n",
      "jump - jump: VERB\n",
      "over - over: ADP\n",
      "the - the: DET\n",
      "lazy - lazy: ADJ\n",
      "dog - dog: NOUN\n",
      ". - .: PUNCT\n",
      "Natural - Natural: PROPN\n",
      "Language - Language: PROPN\n",
      "Processing - processing: NOUN\n",
      "is - be: AUX\n",
      "fascinating - fascinating: ADJ\n",
      "! - !: PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* Identify the POS tags for \"quick,\" \"jumps,\" and \"is.\"\n",
    "    - The POS tag for \"quick\" is ADJ, since it's an adjective that describes the noun \"fox.\"\n",
    "    - The POS tag for \"jumps\" is VERB, because it's the main action in the sentence.\n",
    "    - The POS tag for \"is\" is AUX (auxiliary verb), as it helps connect the subject to the description \"fascinating.\"\n",
    "* Why might POS tagging be useful for tasks like grammar checking or machine translation?\n",
    "    - POS tagging can be useful for grammar checking because it helps catch sentence structure issues, like when someone uses a noun where a verb should be, or forms a sentence in a way that doesn't quite make sense. It can guide tools to suggest better phrasing and help users catch any mistake. For machine translation, it helps the system understand the role each word plays in a sentence. This way, it can recreate that same structure in another language and make the translation be grammatically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama: PERSON (People, including fictional)\n",
      "44th: ORDINAL (\"first\", \"second\", etc.)\n",
      "the United States: GPE (Countries, cities, states)\n",
      "Hawaii: GPE (Countries, cities, states)\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
    ")\n",
    "\n",
    "# Apply the spaCy pipeline to the text\n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* Which entities are recognized by spaCy? (Hint: Loop through doc.ents) \n",
    "    - SpaCy recognizes Barack Obama as a person, '44th' as an ordinal, and 'United States' and 'Hawaii' as a Geopolitical Entity. \n",
    "* What entity types are assigned to \"Barack Obama\" and \"Hawaii\"? (Hint: Use token.label_ properties)\n",
    "    - Barack Obama is assigned as a Person, which is correct, and Hawaii is assigned as a GPE, which includes countries, so it is also accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean version - Named Entities:\n",
      "NLP: ORG (Companies, agencies, institutions, etc.)\n",
      "Sabrina: PERSON (People, including fictional)\n",
      "TDS: ORG (Companies, agencies, institutions, etc.)\n",
      "the Computing & Digital Technologies: ORG (Companies, agencies, institutions, etc.)\n",
      " Tokens: ['After', 'solving', 'a', 'tricky', 'NLP', 'script', 'in', 'computing', 'class', ',', 'Sabrina', 'went', 'to', 'the', 'TDS', 'open', 'house', '.', 'She', 'met', 'with', 'faculty', ',', 'explored', 'cool', 'projects', ',', 'and', 'talked', 'about', 'her', 'plans', 'to', 'continue', 'the', 'Computing', '&', 'Digital', 'Technologies', 'minor', 'with', 'excitement', '!']\n",
      "After - after: ADP\n",
      "solving - solve: VERB\n",
      "a - a: DET\n",
      "tricky - tricky: ADJ\n",
      "NLP - NLP: PROPN\n",
      "script - script: NOUN\n",
      "in - in: ADP\n",
      "computing - compute: VERB\n",
      "class - class: NOUN\n",
      ", - ,: PUNCT\n",
      "Sabrina - Sabrina: PROPN\n",
      "went - go: VERB\n",
      "to - to: ADP\n",
      "the - the: DET\n",
      "TDS - TDS: PROPN\n",
      "open - open: ADJ\n",
      "house - house: NOUN\n",
      ". - .: PUNCT\n",
      "She - she: PRON\n",
      "met - meet: VERB\n",
      "with - with: ADP\n",
      "faculty - faculty: NOUN\n",
      ", - ,: PUNCT\n",
      "explored - explore: VERB\n",
      "cool - cool: ADJ\n",
      "projects - project: NOUN\n",
      ", - ,: PUNCT\n",
      "and - and: CCONJ\n",
      "talked - talk: VERB\n",
      "about - about: ADP\n",
      "her - her: PRON\n",
      "plans - plan: NOUN\n",
      "to - to: PART\n",
      "continue - continue: VERB\n",
      "the - the: DET\n",
      "Computing - Computing: PROPN\n",
      "& - &: CCONJ\n",
      "Digital - Digital: PROPN\n",
      "Technologies - Technologies: PROPN\n",
      "minor - minor: ADJ\n",
      "with - with: ADP\n",
      "excitement - excitement: NOUN\n",
      "! - !: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Original - Clean Text \n",
    "original_text = (\n",
    "    \"After solving a tricky NLP script in computing class, Sabrina went to the TDS open house. \"\n",
    "    \"She met with faculty, explored cool projects, and talked about her plans to continue the Computing & Digital Technologies minor with excitement!\"\n",
    ")\n",
    "\n",
    "# Apply the spaCy pipeline to the text\n",
    "doc_clean = nlp(original_text)\n",
    "\n",
    "\n",
    "# Entities\n",
    "print(\"Clean version - Named Entities:\")\n",
    "for ent in doc_clean.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "# Extract and print token list \n",
    "tokens_spacy = [token.text for token in doc_clean]\n",
    "print(\" Tokens:\", tokens_spacy)\n",
    "\n",
    "for token in doc_clean:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modified version - Named Entities:\n",
      "NLP: ORG (Companies, agencies, institutions, etc.)\n",
      "Computing: PERSON (People, including fictional)\n",
      "TDS: ORG (Companies, agencies, institutions, etc.)\n",
      "Computting &: ORG (Companies, agencies, institutions, etc.)\n",
      "Technolgies: PRODUCT (Objects, vehicles, foods, etc. (not services))\n",
      " Tokens: ['Aftr', 'solving', '...', 'a', 'triky', 'NLP', 'skript', 'in', 'Computing', 'Class', '!', '!', ' ', ',', 'sabrina', 'went', 'to', 'the', 'TDS', 'opnhouse', '.', 'she', 'met', 'with', 'faculty', ',', 'explored', 'cool', 'projects', ',', 'and', 'talked', 'abut', 'her', 'plans', 'to', 'continue', 'the', 'Computting', '&', 'digital', 'Technolgies', 'minor', 'with', 'excitment', '!']\n",
      "Aftr - Aftr: PROPN\n",
      "solving - solving: NOUN\n",
      "... - ...: PUNCT\n",
      "a - a: DET\n",
      "triky - triky: NOUN\n",
      "NLP - NLP: PROPN\n",
      "skript - skript: NOUN\n",
      "in - in: ADP\n",
      "Computing - Computing: PROPN\n",
      "Class - Class: PROPN\n",
      "! - !: PUNCT\n",
      "! - !: PUNCT\n",
      "  -  : SPACE\n",
      ", - ,: PUNCT\n",
      "sabrina - sabrina: PROPN\n",
      "went - go: VERB\n",
      "to - to: ADP\n",
      "the - the: DET\n",
      "TDS - TDS: PROPN\n",
      "opnhouse - opnhouse: NOUN\n",
      ". - .: PUNCT\n",
      "she - she: PRON\n",
      "met - meet: VERB\n",
      "with - with: ADP\n",
      "faculty - faculty: NOUN\n",
      ", - ,: PUNCT\n",
      "explored - explore: VERB\n",
      "cool - cool: ADJ\n",
      "projects - project: NOUN\n",
      ", - ,: PUNCT\n",
      "and - and: CCONJ\n",
      "talked - talk: VERB\n",
      "abut - abut: CCONJ\n",
      "her - her: PRON\n",
      "plans - plan: VERB\n",
      "to - to: PART\n",
      "continue - continue: VERB\n",
      "the - the: DET\n",
      "Computting - Computting: PROPN\n",
      "& - &: CCONJ\n",
      "digital - digital: PROPN\n",
      "Technolgies - Technolgies: PROPN\n",
      "minor - minor: ADJ\n",
      "with - with: ADP\n",
      "excitment - excitment: NOUN\n",
      "! - !: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Modified (typos + weird punctuation + lowercase names)\n",
    "modified_text = (\n",
    "    \"Aftr solving... a triky NLP skript in Computing Class!!  ,sabrina went to the TDS opnhouse. \"\n",
    "    \"she met with faculty, explored cool projects, and talked abut her plans to continue the Computting & digital Technolgies minor with excitment!\"\n",
    ")\n",
    "\n",
    "# Apply the spaCy pipeline to the text\n",
    "doc_modified = nlp(modified_text)\n",
    "\n",
    "# Print entities from modified version\n",
    "print(\" Modified version - Named Entities:\")\n",
    "for ent in doc_modified.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "# Extract and print token list \n",
    "tokens_spacy = [token.text for token in doc_modified]\n",
    "print(\" Tokens:\", tokens_spacy)\n",
    "\n",
    "for token in doc_modified:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* How does spaCy handle your modifications?\n",
    "    - SpaCy struggles when the input contains typos, weird punctuation, or inconsistent capitalization. In the clean version, it correctly identified \"Sabrina\" as a PERSON, \"TDS\" as an ORG,and \"Computing & Digital Technologies\" as part of an ORG. But in the modified version, I removed the capitalization from \"Sabrina\" , so while it was still tagged as PROPN (proper noun), it was no longer recognized as a named entity. I also introduced a typo in \"Technologies\" (misspelled as \"Technolgies\") and removed the capital letter from \"Digital.\"As a result, spaCy incorrectly labeled \"Technolgies\" as a PRODUCT, showing that spelling and formatting errors confuse the model. \n",
    "\n",
    "* Did any entities or tags change? Why might this happen?\n",
    "    -  Yes , several entity tags and part-of-speech (POS) tags changed or disappeared. For example, \"Computing & Digital Technologies\" got split into \"Computting &\" (ORG) and \"Technolgies\" (PRODUCT) in the modified version. Also, the POS tag for \"solving\" changed from VERB to NOUN, likely because the  context was confusing for spaCy to recognize it as an action. Punctuation like '!' and '...' were still tokenized correctly, but added noise that may have impacted interpretation.This shows that spaCy relies on specific patters, even small modifications can lead to misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
