{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK 7 \n",
    "### Elements of Computing \n",
    "### Sabrina Cohen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens: ['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n",
      "\n",
      " Token | Lemma | Head | Morph:\n",
      "The - Lemma: the | Head: fox | Morph: Definite=Def|PronType=Art\n",
      "quick - Lemma: quick | Head: fox | Morph: Degree=Pos\n",
      "brown - Lemma: brown | Head: fox | Morph: Degree=Pos\n",
      "fox - Lemma: fox | Head: jump | Morph: Number=Sing\n",
      "does - Lemma: do | Head: jump | Morph: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "n't - Lemma: not | Head: jump | Morph: Polarity=Neg\n",
      "jump - Lemma: jump | Head: jump | Morph: VerbForm=Inf\n",
      "over - Lemma: over | Head: jump | Morph: \n",
      "the - Lemma: the | Head: dog | Morph: Definite=Def|PronType=Art\n",
      "lazy - Lemma: lazy | Head: dog | Morph: Degree=Pos\n",
      "dog - Lemma: dog | Head: over | Morph: Number=Sing\n",
      ". - Lemma: . | Head: jump | Morph: PunctType=Peri\n",
      "Natural - Lemma: Natural | Head: Language | Morph: Number=Sing\n",
      "Language - Lemma: Language | Head: Processing | Morph: Number=Sing\n",
      "Processing - Lemma: processing | Head: is | Morph: Number=Sing\n",
      "is - Lemma: be | Head: is | Morph: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "fascinating - Lemma: fascinating | Head: is | Morph: Degree=Pos\n",
      "! - Lemma: ! | Head: is | Morph: PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Input text\n",
    "text = (\n",
    "    \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
    ")\n",
    "\n",
    "# Apply the spaCy pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and print token list (basic)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(\" Tokens:\", tokens_spacy)\n",
    "\n",
    "# Detailed token analysis\n",
    "print (\"\\n Token | Lemma | Head | Morph:\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - Lemma: {token.lemma_} | Head: {token.head.text} | Morph: {token.morph}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* How does spaCy process the various tokens? (Hint: Loop through the doc container using the token attributes: .text_, .head, .lemma_, .morph.)\n",
    "    - SpaCy processes the text by breaking it down into the individual words, which are known as token objects. Each token has unique attributes, and analyzing them alllows us to figure out the purpose anfd characteristics of each word within that sentence.  In this case, the code displayed the following attributes:\n",
    "        - Text: The entity as it appears in the text.\n",
    "        - Lemma: The base/dictionary form of a word.\n",
    "        - Head: The main word another word depends on.\n",
    "        - Morph: Grammatical features like tense, number, or gender.\n",
    "* How does spaCy handle punctuation marks like periods and commas?\n",
    "- SpaCy handles punctuation marks like their own token, the same way as words are listed. This is important as it also analyzes these small marks, and displays their unique attributes - \n",
    "even though they aren't complete words. \n",
    "* What happens when the text includes contractions (e.g., \"don't\")?\n",
    "-   When looking at how spaCy proesses the various tokens, it is clear that it separates contractions into two. In this case, one token was 'does' and the other was 'n't'. If analyzed closely, the 'n't' token is processed as a negation, so NLP understands the grammatical structure and accurately analyzes this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Part-of-Speech Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - the: DET\n",
      "quick - quick: ADJ\n",
      "brown - brown: ADJ\n",
      "fox - fox: NOUN\n",
      "does - do: AUX\n",
      "n't - not: PART\n",
      "jump - jump: VERB\n",
      "over - over: ADP\n",
      "the - the: DET\n",
      "lazy - lazy: ADJ\n",
      "dog - dog: NOUN\n",
      ". - .: PUNCT\n",
      "Natural - Natural: PROPN\n",
      "Language - Language: PROPN\n",
      "Processing - processing: NOUN\n",
      "is - be: AUX\n",
      "fascinating - fascinating: ADJ\n",
      "! - !: PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* Identify the POS tags for \"quick,\" \"jumps,\" and \"is.\"\n",
    " - The POS tag for \"quick\" is ADJ, since it's an adjective that describes the noun \"fox.\"\n",
    " - The POS tag for \"jumps\" is VERB, because it's the main action in the sentence.\n",
    " - The POS tag for \"is\" is AUX (auxiliary verb), as it helps connect the subject to the description \"fascinating.\"\n",
    "* Why might POS tagging be useful for tasks like grammar checking or machine translation?\n",
    "- POS tagging can be useful for grammar checking because it helps catch sentence structure issues, like when someone uses a noun where a verb should be, or forms a sentence in a way that doesn't quite make sense. It can guide tools to suggest better phrasing and help users catch any mistake. For machine translation, it helps the system understand the role each word plays in a sentence. This way, it can recreate that same structure in another language and make the translation be grammatically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama: PERSON (People, including fictional)\n",
      "44th: ORDINAL (\"first\", \"second\", etc.)\n",
      "the United States: GPE (Countries, cities, states)\n",
      "Hawaii: GPE (Countries, cities, states)\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Answer: \n",
    "* Which entities are recognized by spaCy? (Hint: Loop through doc.ents) \n",
    "- SpaCy recognizes Barack Obama as a person, '44th' as an ordinal, and 'United States' and 'Hawaii' as a Geopolitical Entity. \n",
    "* What entity types are assigned to \"Barack Obama\" and \"Hawaii\"? (Hint: Use token.label_ properties)\n",
    "- Barack Obama is assigned as a Person, which is correct, and Hawaii is assigned as a GPE, which includes countries, so it is also accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP: ORG (Companies, agencies, institutions, etc.)\n",
      "Sabrina: PERSON (People, including fictional)\n",
      "TDS: ORG (Companies, agencies, institutions, etc.)\n",
      "the Computing & Digital Technologies: ORG (Companies, agencies, institutions, etc.)\n",
      "['After', 'solving', 'a', 'tricky', 'NLP', 'script', 'in', 'computing', 'class', ',', 'Sabrina', 'went', 'to', 'the', 'TDS', 'open', 'house', '.', 'She', 'met', 'with', 'faculty', ',', 'explored', 'cool', 'projects', ',', 'and', 'talked', 'about', 'her', 'plans', 'to', 'continue', 'the', 'Computing', '&', 'Digital', 'Technologies', 'minor', 'with', 'excitement', '!']\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"After solving a tricky NLP script in computing class, Sabrina went to the TDS open house. She met with faculty, explored cool projects, \"\n",
    "    \"and talked about her plans to continue the Computing & Digital Technologies minor with excitement!\"\n",
    ")\n",
    "\n",
    "# Run the spaCy pipeline \n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "\n",
    "# Apply the NLP pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(tokens_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP: ORG (Companies, agencies, institutions, etc.)\n",
      "TDS: ORG (Companies, agencies, institutions, etc.)\n",
      "the Computting & Digital Technolgies: ORG (Companies, agencies, institutions, etc.)\n",
      "['Aftr', 'solving', 'a', 'triky', 'NLP', 'skript', 'in', 'computing', 'clas', ',', ' ', 'sabrina', 'went', 'to', 'the', 'TDS', 'opn', 'house', '.', 'She', 'met', 'with', 'faclty', ',', 'explord', 'cool', 'projekts', ',', 'and', 'tlked', 'abut', 'her', 'plans', 'to', 'continnu', 'the', 'Computting', '&', 'Digital', 'Technolgies', 'minor', 'with', 'excitment', '!']\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "     \"Aftr solving a triky NLP skript in computing clas,  sabrina went to the TDS opn house. \"\n",
    "    \"She met with faclty, explord cool projekts, and tlked abut her plans to continnu the Computting & Digital Technolgies minor with excitment!\"\n",
    ")\n",
    "\n",
    "# Run the spaCy pipeline \n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "\n",
    "# Apply the NLP pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(tokens_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
